{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aaf1bd5",
   "metadata": {},
   "source": [
    "## <center> Image recognition </center>\n",
    "### <center> Train your own model vs Transfer learning </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "688cb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.utils import to_categorical, load_img\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications import vgg16, resnet\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59167aa2",
   "metadata": {},
   "source": [
    "### I. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f3b8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e02993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "X_test shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8f04d",
   "metadata": {},
   "source": [
    "- CIFAR10 is 60K image split into 50K on train and 10 on test. Each image size is: 32 x 32 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cec848",
   "metadata": {},
   "source": [
    "### II. Preprocessing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10346e3a",
   "metadata": {},
   "source": [
    "#### 1. Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf8745",
   "metadata": {},
   "source": [
    "First, let's convert our data into float variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd721be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f2e7b",
   "metadata": {},
   "source": [
    "Next, let's divide by 255 to normalize data between 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ebad3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9402ad0",
   "metadata": {},
   "source": [
    "#### 2. Convert labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47eb57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7ef26",
   "metadata": {},
   "source": [
    "### III. Trained from scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f5499",
   "metadata": {},
   "source": [
    "#### 1. Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbd598ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First convolution block\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(32, 32, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second convolution block\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Third convolution block\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten then dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "# Output layer\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Model compilation\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Tensorboard logging\n",
    "logger = TensorBoard(log_dir='logs/3Conv_blocks', write_graph=True, histogram_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "384c03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,034\n",
      "Trainable params: 181,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792deda",
   "metadata": {},
   "source": [
    "#### 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ee8a409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 41s 24ms/step - loss: 1.6862 - accuracy: 0.3730 - val_loss: 1.3368 - val_accuracy: 0.5086\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 1.3113 - accuracy: 0.5258 - val_loss: 1.1875 - val_accuracy: 0.5671\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 1.1673 - accuracy: 0.5820 - val_loss: 1.0401 - val_accuracy: 0.6371\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 1.0709 - accuracy: 0.6189 - val_loss: 0.9487 - val_accuracy: 0.6663\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.0093 - accuracy: 0.6409 - val_loss: 0.8926 - val_accuracy: 0.6870\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.9640 - accuracy: 0.6590 - val_loss: 0.8352 - val_accuracy: 0.7073\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.9208 - accuracy: 0.6737 - val_loss: 0.8316 - val_accuracy: 0.7061\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.8986 - accuracy: 0.6803 - val_loss: 0.7951 - val_accuracy: 0.7239\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8695 - accuracy: 0.6916 - val_loss: 0.7771 - val_accuracy: 0.7286\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 0.8510 - accuracy: 0.7001 - val_loss: 0.7665 - val_accuracy: 0.7363\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.8317 - accuracy: 0.7067 - val_loss: 0.7948 - val_accuracy: 0.7232\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.8225 - accuracy: 0.7098 - val_loss: 0.7652 - val_accuracy: 0.7387\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.8048 - accuracy: 0.7175 - val_loss: 0.7438 - val_accuracy: 0.7405\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.7945 - accuracy: 0.7185 - val_loss: 0.7340 - val_accuracy: 0.7427\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7790 - accuracy: 0.7258 - val_loss: 0.7314 - val_accuracy: 0.7460\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.7820 - accuracy: 0.7267 - val_loss: 0.7220 - val_accuracy: 0.7489\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.7615 - accuracy: 0.7331 - val_loss: 0.7043 - val_accuracy: 0.7531\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.7555 - accuracy: 0.7345 - val_loss: 0.7524 - val_accuracy: 0.7397\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.7447 - accuracy: 0.7371 - val_loss: 0.7050 - val_accuracy: 0.7568\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.7398 - accuracy: 0.7414 - val_loss: 0.7002 - val_accuracy: 0.7585\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.7397 - accuracy: 0.7391 - val_loss: 0.7346 - val_accuracy: 0.7386\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7302 - accuracy: 0.7418 - val_loss: 0.6936 - val_accuracy: 0.7616\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.7240 - accuracy: 0.7462 - val_loss: 0.6887 - val_accuracy: 0.7630\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7191 - accuracy: 0.7485 - val_loss: 0.6825 - val_accuracy: 0.7661\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7101 - accuracy: 0.7513 - val_loss: 0.6975 - val_accuracy: 0.7628\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7053 - accuracy: 0.7516 - val_loss: 0.7040 - val_accuracy: 0.7595\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7097 - accuracy: 0.7521 - val_loss: 0.6646 - val_accuracy: 0.7726\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.6987 - accuracy: 0.7562 - val_loss: 0.6798 - val_accuracy: 0.7647\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7006 - accuracy: 0.7525 - val_loss: 0.6869 - val_accuracy: 0.7631\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6967 - accuracy: 0.7566 - val_loss: 0.6620 - val_accuracy: 0.7720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc19e6abc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a49ea1",
   "metadata": {},
   "source": [
    "#### 3. Saving the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "180101dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5872"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_structure = model.to_json()\n",
    "f = Path('model_structure.json')\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b544fe",
   "metadata": {},
   "source": [
    "#### 4. Saving the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d858eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa049eb2",
   "metadata": {},
   "source": [
    "#### 5. Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1d5a4",
   "metadata": {},
   "source": [
    "- Loading the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57fafe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('model_structure.json')\n",
    "model_structure = p.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bd2dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_from_json(model_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed99dc",
   "metadata": {},
   "source": [
    "- Loading the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e108a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfffd37",
   "metadata": {},
   "source": [
    "#### 6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5246d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6620 - accuracy: 0.7720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.662005603313446, 0.7720000147819519]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb9745",
   "metadata": {},
   "source": [
    "#### 7. Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c5967",
   "metadata": {},
   "source": [
    "- Loading external images to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f3ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "frog = load_img(\"test_images/frog.png\", target_size=(32, 32))\n",
    "bay = load_img(\"test_images/bay.jpg\", target_size=(32, 32))\n",
    "dog = load_img(\"test_images/dog.png\", target_size=(32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff3b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    \"Plane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Boat\",\n",
    "    \"Truck\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410df82b",
   "metadata": {},
   "source": [
    "- Predicting the 3 test image's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f4a2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step\n",
      "['Frog', 'Car', 'Car']\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.stack([frog, bay, dog], axis=0))\n",
    "pred_classes = []\n",
    "for prob in pred:\n",
    "    most_likely = int(np.argmax(prob))\n",
    "    class_label = class_labels[most_likely]\n",
    "    pred_classes.append(class_label)\n",
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce604b2",
   "metadata": {},
   "source": [
    "### IV. Transfer Learning - VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe1704",
   "metadata": {},
   "source": [
    "#### 0. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13949349",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc8a3e",
   "metadata": {},
   "source": [
    "#### 1. Pretrained VGG16 without output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ffb376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = vgg16.VGG16(weights=\"imagenet\",\n",
    "                               include_top=False,\n",
    "                               input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10714747",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d6890b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vgg16.preprocess_input(X_train)\n",
    "X_test = vgg16.preprocess_input(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff144c8",
   "metadata": {},
   "source": [
    "#### 3. Extracting images fautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "276ba72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 236s 150ms/step\n",
      "313/313 [==============================] - 60s 185ms/step\n"
     ]
    }
   ],
   "source": [
    "features_train = pretrained_model.predict(X_train)\n",
    "features_test = pretrained_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6c12c",
   "metadata": {},
   "source": [
    "#### 3. Fine-tuning the model to our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f49af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=features_train.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5aa1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Tensorboard logging\n",
    "logger = TensorBoard(log_dir='logs/fined-tuned-vgg16', write_graph=True, histogram_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7766c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 2.4413 - accuracy: 0.4611 - val_loss: 1.1856 - val_accuracy: 0.5855\n",
      "Epoch 2/15\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2865 - accuracy: 0.5595 - val_loss: 1.0965 - val_accuracy: 0.6156\n",
      "Epoch 3/15\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2033 - accuracy: 0.5857 - val_loss: 1.0596 - val_accuracy: 0.6403\n",
      "Epoch 4/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1632 - accuracy: 0.6042 - val_loss: 1.0461 - val_accuracy: 0.6473\n",
      "Epoch 5/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1277 - accuracy: 0.6187 - val_loss: 1.0518 - val_accuracy: 0.6479\n",
      "Epoch 6/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1086 - accuracy: 0.6247 - val_loss: 1.0427 - val_accuracy: 0.6512\n",
      "Epoch 7/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0852 - accuracy: 0.6285 - val_loss: 1.0404 - val_accuracy: 0.6483\n",
      "Epoch 8/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0678 - accuracy: 0.6370 - val_loss: 1.0513 - val_accuracy: 0.6546\n",
      "Epoch 9/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0437 - accuracy: 0.6453 - val_loss: 1.0297 - val_accuracy: 0.6623\n",
      "Epoch 10/15\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0255 - accuracy: 0.6438 - val_loss: 1.0528 - val_accuracy: 0.6564\n",
      "Epoch 11/15\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0142 - accuracy: 0.6520 - val_loss: 1.0691 - val_accuracy: 0.6529\n",
      "Epoch 12/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9914 - accuracy: 0.6570 - val_loss: 1.0838 - val_accuracy: 0.6571\n",
      "Epoch 13/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9835 - accuracy: 0.6619 - val_loss: 1.0729 - val_accuracy: 0.6646\n",
      "Epoch 14/15\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 0.9742 - accuracy: 0.6621 - val_loss: 1.0696 - val_accuracy: 0.6595\n",
      "Epoch 15/15\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9571 - accuracy: 0.6695 - val_loss: 1.0821 - val_accuracy: 0.6586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b612d59e40>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_train,\n",
    "          y_train,\n",
    "          validation_data = (features_test, y_test),\n",
    "          epochs=15,\n",
    "          callbacks = [logger],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84754724",
   "metadata": {},
   "source": [
    "### IV. Transfer Learning - Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d86cbd",
   "metadata": {},
   "source": [
    "#### 0. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dfeff227",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd991e2d",
   "metadata": {},
   "source": [
    "#### 1. Pretrained ResNet50 without output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "df480440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = resnet.ResNet50(weights=\"imagenet\",\n",
    "                               include_top=False,\n",
    "                               input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a7c6b",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efc99c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = resnet.preprocess_input(X_train)\n",
    "X_test = resnet.preprocess_input(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f0450",
   "metadata": {},
   "source": [
    "#### 3. Extracting images fautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "94583fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 151s 93ms/step\n",
      "313/313 [==============================] - 34s 97ms/step\n"
     ]
    }
   ],
   "source": [
    "features_train = pretrained_model.predict(X_train)\n",
    "features_test = pretrained_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffd95d",
   "metadata": {},
   "source": [
    "#### 3. Fine-tuning the model to our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7633598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=features_train.shape[1:]))\n",
    "model.add(Dense(528, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2438f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Tensorboard logging\n",
    "logger = TensorBoard(log_dir='logs/fined-tuned-resnet50', write_graph=True, histogram_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fa595956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1563 [==============================] - 17s 9ms/step - loss: 1.5684 - accuracy: 0.4824 - val_loss: 1.1511 - val_accuracy: 0.6166\n",
      "Epoch 2/15\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2582 - accuracy: 0.5698 - val_loss: 1.0787 - val_accuracy: 0.6385\n",
      "Epoch 3/15\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1951 - accuracy: 0.5981 - val_loss: 1.0592 - val_accuracy: 0.6426\n",
      "Epoch 4/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.1509 - accuracy: 0.6112 - val_loss: 1.0396 - val_accuracy: 0.6509\n",
      "Epoch 5/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.1153 - accuracy: 0.6218 - val_loss: 1.0364 - val_accuracy: 0.6456\n",
      "Epoch 6/15\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.0923 - accuracy: 0.6324 - val_loss: 1.0190 - val_accuracy: 0.6515\n",
      "Epoch 7/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.0705 - accuracy: 0.6421 - val_loss: 1.0264 - val_accuracy: 0.6552\n",
      "Epoch 8/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.0417 - accuracy: 0.6509 - val_loss: 1.0062 - val_accuracy: 0.6560\n",
      "Epoch 9/15\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.0173 - accuracy: 0.6586 - val_loss: 1.0031 - val_accuracy: 0.6593\n",
      "Epoch 10/15\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9945 - accuracy: 0.6676 - val_loss: 0.9858 - val_accuracy: 0.6639\n",
      "Epoch 11/15\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9756 - accuracy: 0.6753 - val_loss: 0.9953 - val_accuracy: 0.6670\n",
      "Epoch 12/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9546 - accuracy: 0.6802 - val_loss: 0.9924 - val_accuracy: 0.6676\n",
      "Epoch 13/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9318 - accuracy: 0.6867 - val_loss: 0.9937 - val_accuracy: 0.6650\n",
      "Epoch 14/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9126 - accuracy: 0.6954 - val_loss: 1.0187 - val_accuracy: 0.6611\n",
      "Epoch 15/15\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8992 - accuracy: 0.7008 - val_loss: 0.9929 - val_accuracy: 0.6688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b60baf4790>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_train,\n",
    "          y_train,\n",
    "          validation_data = (features_test, y_test),\n",
    "          epochs=15,\n",
    "          callbacks = [logger],\n",
    "          shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
